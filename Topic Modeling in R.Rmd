---
title: "Text Analysis"
author: "Imran"
date: "12/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# install packages

```{r}
install.packages("tm")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("wordcloud")
install.packages("pals")
install.packages("SnowballC")
install.packages("lda")
install.packages("kableExtra")
install.packages("flextable")
# install klippy for the purpose of copy-to-clipboard button in code chunks
remotes::install_github("rlesur/klippy")
```
```{r}
# loading libraries
options(stringsAsFactors = F)
options("scipen" = 100, "digits" = 4)

# load packages
library(knitr) 
library(kableExtra) 
library(DT)
library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(lda)
library(flextable)
# activate klippy for copy-to-clipboard button
klippy::klippy()
```

```{r}
# loading data set
textdata <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb"))
# loading stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# creating corpus object
corpus <- Corpus(DataframeSource(textdata))
# Pre-processing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)


```

```{r}
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
dim(DTM)
```

```{r}
# DTM and metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
```

```{r}
#number of topics
K <- 16
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))
```
```{r}
tmResult <- posterior(topicModel)

# resulting object format
attributes(tmResult)
nTerms(DTM)  

# topics are probability distribtions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta)
rowSums(beta)
theta <- tmResult$topics 
dim(theta)
rowSums(theta)[1:10]
terms(topicModel, 10)
```

```{r}
exampleTermData <- terms(topicModel, 12)
exampleTermData[, 1:10]
```

```{r}  
## selecting top 6 topics
top6termsPerTopic <- terms(topicModel, 6)
topicNames <- apply(top6termsPerTopic, 2, paste, collapse=" ")
```

```{r}
# visualization as word cloud
topicToViz <- 15 # topic of interest
topicToViz <- grep('mexico', topicNames)[1]
# select to 35 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top35terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:35]
words <- names(top35terms)

# extract the probabilites of each of the 35 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:35]

# visualizing as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```
```{r}
## lets look the content of 3 sample documents
exampleIds <- c(2, 100, 200)
lapply(corpus[exampleIds], as.character)
```
```{r}
exampleIds <- c(2, 100, 200)
print(paste0(exampleIds[1], ": ", substr(content(corpus[[exampleIds[1]]]), 0, 500), '...'))
print(paste0(exampleIds[2], ": ", substr(content(corpus[[exampleIds[2]]]), 0, 350), '...'))
print(paste0(exampleIds[3], ": ", substr(content(corpus[[exampleIds[3]]]), 0, 400), '...'))
```

```{r}
N <- length(exampleIds)
# getting topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)

```
```{r}
attr(topicModel, "alpha")
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 550, verbose = 30, alpha = 0.2))
```
```{r}
## resetting topics names
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 5), 2, paste, collapse = " ")
```

```{r}
##visualization of topics again on the basis of their relative probabilities
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document = factor(1:N)), variable.name = "topic", id.vars = "document")  
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```

